{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Codigo de norving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     16,
     20,
     24,
     28,
     32,
     42,
     48
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Spelling Corrector in Python 3; see http://norvig.com/spell-correct.html\n",
    "\n",
    "    Copyright (c) 2007-2016 Peter Norvig\n",
    "    MIT license: www.opensource.org/licenses/mit-license.php\n",
    "\"\"\"\n",
    "################ Spelling Corrector ################\n",
    "####################################################\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "################ Test Code \n",
    "\n",
    "def unit_tests():\n",
    "    assert correction('speling') == 'spelling','Err: insert'# insert\n",
    "    assert correction('korrectud') == 'corrected'           # replace 2\n",
    "    assert correction('bycycle') == 'bicycle'               # replace\n",
    "    assert correction('inconvient') == 'inconvenient'       # insert 2\n",
    "    assert correction('arrainged') == 'arranged'            # delete\n",
    "    assert correction('peotry') =='poetry'                  # transpose\n",
    "    assert correction('peotryy') =='poetry'                 # transpose + delete\n",
    "    assert correction('word') == 'word'                     # known\n",
    "    assert correction('quintessential') == 'quintessential' # unknown\n",
    "    assert words('This is a TEST.') == ['this', 'is', 'a', 'test']\n",
    "    assert Counter(words('This is a test. 123; A TEST this is.')) == (\n",
    "           Counter({'123': 1, 'a': 2, 'is': 2, 'test': 2, 'this': 2}))\n",
    "    assert len(WORDS) == 32198\n",
    "    assert sum(WORDS.values()) == 1115585\n",
    "    assert WORDS.most_common(10) == [\n",
    "        ('the', 79809),\n",
    "        ('of', 40024),\n",
    "        ('and', 38312),\n",
    "        ('to', 28765),\n",
    "        ('in', 22023),\n",
    "        ('a', 21124),\n",
    "        ('that', 12512),\n",
    "        ('he', 12401),\n",
    "        ('was', 11410),\n",
    "        ('it', 10681)]\n",
    "    assert WORDS['the'] == 79809\n",
    "    assert P('quintessential') == 0\n",
    "    assert 0.07 < P('the') < 0.08\n",
    "    return 'unit_tests pass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit_tests pass\n",
      "spelling\n",
      "corrected\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "print(unit_tests())\n",
    "print(correction('speling'))\n",
    "print(correction('korrectud'))\n",
    "print(correction('thu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.La siguiente palabra m\\'as probable\n",
    "\n",
    "Usando _big.txt_ crear una funci\\'on que estime la siguiente palabra m\\'as probable dada una anterior. La funci\\'on debbe calcular \n",
    "    $$w_{i+1} = \\text{argmax}_{w_{i+1}}P(W_{i+1}|w_i)$$\n",
    "Para este trabajo\n",
    "1. Podemos asumir que ambas palabras siempre existir\\'an en la colecci\\'on\n",
    "2. Requerimos una funci\\'on similar a $P$, que calcule $P(w_1|w_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     4,
     18,
     22
    ]
   },
   "outputs": [],
   "source": [
    "################################\n",
    "### Funciones para trabajar  ###\n",
    "################################\n",
    "\n",
    "def words_from_file( fileName ):\n",
    "    \"\"\" Obtenemos las palabras de un archivo. \"\"\"\n",
    "    file = open(fileName).read()\n",
    "    return re.findall(r'\\w+', file.lower())\n",
    "\n",
    "def create_dict(texto):\n",
    "    \"\"\" Funcion para crear el diccionario auxiliar para \n",
    "    calcular las probabilidades necesarias. \n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    for i in range(1,len(texto)):\n",
    "        if texto[i] not in ret:\n",
    "            ret[texto[i]] = {}\n",
    "        if texto[i-1] not in ret[texto[i]]:\n",
    "            (ret[texto[i]])[texto[i-1]] = 0\n",
    "            \n",
    "        (ret[texto[i]])[texto[i-1]] += 1\n",
    "    return ret\n",
    "\n",
    "def prob_cond(a, b, dic):\n",
    "    \"\"\" Probabilidad de A dado B en funcion de dic \"\"\"\n",
    "    try:\n",
    "        return ((dic[a])[b])/sum(dic[b].values())\n",
    "    except KeyError:\n",
    "        return -1\n",
    "\n",
    "def next_word(word, dic):\n",
    "    \"\"\" Obtenemos la siguiente palabra mas probable en funcion\n",
    "    del dicionario y sus probabiliodades. \"\"\"\n",
    "    maximo = ('Err', -inf)\n",
    "    for key in dic[word]:\n",
    "        prob = prob_cond(key, word, dic)\n",
    "        if prob > maximo[1]:\n",
    "            maximo = (key, prob)\n",
    "    return maximo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new york\n",
      "0.15811258278145696\n"
     ]
    }
   ],
   "source": [
    "dic = create_dict(words_from_file('big.txt'))\n",
    "word = 'new'\n",
    "\n",
    "print( word +' '+next_word( word, dic) )\n",
    "print( prob_cond('york','new', dic) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.Aqu\\'i la maquina juega al ahorcado\n",
    "Se recomienda extender y mejorar de alg\\'un modo la funci\\'on propuesta por __Norving__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0,
     7,
     8,
     19,
     25,
     31,
     39,
     40,
     42,
     49,
     51,
     56
    ]
   },
   "outputs": [],
   "source": [
    "def under(word):\n",
    "    word = word.split('_')\n",
    "    if len(word) > 5:\n",
    "        print('Demasiadas letras desconocidas')\n",
    "        return None\n",
    "    return word\n",
    "\n",
    "def candidatos(word):\n",
    "    ''' \n",
    "        Recibe a word ya con el 'split' aplicado \n",
    "        y regresamos las posibles palabras '''    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'; n_letters = len(letters)\n",
    "    flag = word[-1] if word[-1] != '' else 'BrendA'\n",
    "    # Creamos los posibles 'pedacitos' de la palabra\n",
    "    words = [ele + letter for ele in word[:len(word)-1] for letter in letters]\n",
    "    # Variables auxiliares\n",
    "    options = words[:n_letters]\n",
    "    options_t = []\n",
    "    # Concatenamos los posibles 'pedacitos'\n",
    "    for k in range( 1, len(words)//n_letters ):\n",
    "        for option in options:\n",
    "            for i in range(n_letters):\n",
    "                options_t.append(option + words[n_letters*k + i])\n",
    "        options = options_t; options_t = []\n",
    "        \n",
    "    if flag != 'BrendA': # Checamos si al final hay un '_' o una letra\n",
    "        for i in range(len(options)): \n",
    "            options[i] = options[i] + flag\n",
    "    # Regresamos unicamente \\'unicamente las palabras que esten en el diccionario\n",
    "    return set(opt for opt in options if opt in WORDS)\n",
    "\n",
    "def dist_lev(source, target):\n",
    "    if source == target: return 0\n",
    "    # Crear matriz\n",
    "    n_s, n_t = len(source), len(target)\n",
    "    dist = [[0 for i in range(n_t+1)] for x in range(n_s+1)]\n",
    "    for i in range(n_s+1): dist[i][0] = i\n",
    "    for j in range(n_t+1): dist[0][j] = j\n",
    "    # Calculando la distancia\n",
    "    for i in range(n_s):\n",
    "        for j in range(n_t):\n",
    "            cost = 0 if source[i] == target[j] else 1\n",
    "            dist[i+1][j+1] = min(\n",
    "                                    dist[i][j+1] + 1,   # deletion\n",
    "                                    dist[i+1][j] + 1,   # insertion\n",
    "                                    dist[i][j] + cost   # substitution\n",
    "                                )\n",
    "    return dist[-1][-1]\n",
    "\n",
    "def closest(word, options):\n",
    "    ret = 'BrendA', inf\n",
    "    for opt in options:\n",
    "        dist = dist_lev(word, opt)\n",
    "        ret = (opt, dist) if dist < ret[1] else ret\n",
    "    return ret\n",
    "    \n",
    "def hangman(word):\n",
    "    options = candidatos( under(word) )\n",
    "    return closest(word, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sherlock\n",
      "notebook\n",
      "hello\n",
      "people\n",
      "philosophy\n",
      "significance\n",
      "knowledge\n"
     ]
    }
   ],
   "source": [
    "print(hangman('s_e_l_c_')[0]) #sherlock\n",
    "print(hangman('no_eb_o_')[0]) #notebook\n",
    "print(hangman('hesignificance__o')[0])    #hello\n",
    "\n",
    "print(hangman('pe_p_e')[0]) #people\n",
    "print(hangman('phi__sop_y')[0]) #philospphy\n",
    "print(hangman('si_nif_c_nc_')[0]) #significance\n",
    "print(hangman('kn__l_d_e')[0])      #sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.Ahorcado al extremo\n",
    "Unir la funci\\'on de _2_ y _2.1_ para, utilizando una palabra de contexto, completar palabras con mayor precisi\\'on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     2,
     4,
     10,
     12,
     22
    ]
   },
   "outputs": [],
   "source": [
    "dic = create_dict('big.txt')\n",
    "\n",
    "def super_under(word):\n",
    "    ct = Counter(word)\n",
    "    if len(word) - ct['_'] < 1:\n",
    "        print('Demasiadas letras desconocidas')\n",
    "        return None\n",
    "    word = word.split('_')\n",
    "    return word\n",
    "\n",
    "def super_closest( context, options):\n",
    "    ret = 'BrendA', -10\n",
    "    for opt in options:  # Buscando el ret adecuado\n",
    "        if known(opt):\n",
    "            # Esta es la misma funcion de probabilidad del ejercicio anterior\n",
    "            prob = prob_cond(opt, context, dic)\n",
    "            #  En caso de que las proabilidades empaten\n",
    "            # utilizamos las distancia entre las palabras\n",
    "            # para responder.\n",
    "            ret = ((opt, prob) if dist_lev(context, opt) < dist_lev(context, ret[0]) else ret) if prob == ret[1] else ret\n",
    "            ret = (opt, prob) if prob > ret[1] else ret               \n",
    "    return ret\n",
    "\n",
    "def super_hangman(context, word):\n",
    "    options = candidatos( super_under(word) )\n",
    "    return super_closest(context, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('holmes', 1.0)\n",
      "('states', 0.7620751341681574)\n",
      "('house', 0.037142857142857144)\n",
      "('york', 0.15811258278145696)\n",
      "('lincoln', 0.6666666666666666)\n"
     ]
    }
   ],
   "source": [
    "print(super_hangman('sherlock', '_____s'))  #holmes\n",
    "print(super_hangman('united', '_t_t__'))    #states\n",
    "print(super_hangman('white', '___s_'))      #house\n",
    "print(super_hangman('new', 'y___'))         #york\n",
    "print(super_hangman('abraham', 'l_____n'))  #lincoln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Correci\\'on ortografica simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "# simple extraction of words\n",
    "def words (text) : \n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# siple loading of the documents\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "def get_texts_from_catdir( catdir ):\n",
    "    texts = [ ]\n",
    "    TARGET_DIR = catdir # \"./target\"\n",
    "    for f_name in sorted( os.listdir( TARGET_DIR )) :\n",
    "        f_path = os.path.join( TARGET_DIR, f_name )\n",
    "        #print(f_name)\n",
    "        #print(f_path)\n",
    "        f = open( f_path , 'r', encoding='utf8' )\n",
    "        #print( f_name )\n",
    "        texts += [ f.read( ) ]\n",
    "        f.close( )\n",
    "    print( '%d files loaded . ' %len(texts) )\n",
    "    return texts\n",
    "\n",
    "# Load the RAW text\n",
    "target_txt = get_texts_from_catdir( './target' )\n",
    "\n",
    "# Print first 10 words in document0\n",
    "print( words(target_txt[0])[:10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mezclar diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     6,
     12,
     14
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "with open('WORDS_IN_NEWS.txt', 'r') as infile: # Exportando WORDS_IN_NEWS\n",
    "    WORDS_IN_NEWS = json.load( infile )\n",
    "WORDS_IN_NEWS = Counter(WORDS_IN_NEWS)\n",
    "\n",
    "WORDS = WORDS + WORDS_IN_NEWS\n",
    "print(WORDS['the'])\n",
    "print(WORDS.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detectar las plabras mal escritas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mispelled_and_candidates( target_words ):\n",
    "    mispelled_candidates = []\n",
    "    for word in target_words:\n",
    "        temp = list(candidates(word))  # candidates de Norving\n",
    "        if len(temp) > 1:\n",
    "            temp.sort(key=lambda x: dist_lev(word, x))\n",
    "            mispelled_candidates.append((word, temp[:10])) #Tomamos las primeras 10\n",
    "    return mispelled_candidates\n",
    "\n",
    "def mispelled_and_candidates( target_words ):\n",
    "    mispelled_candidates = []\n",
    "    \n",
    "    for word in target_words:\n",
    "        candidatos = list(candidates(word))\n",
    "        candidatos.sort(key=lambda x: dist_lev(word, x))\n",
    "        if len(candidatos) > 1:\n",
    "            # En caso de que haya una opcion\n",
    "            mispelled_candidates.append((word, candidatos[:10]))\n",
    "        elif len(candidatos) == 1 and word not in candidatos:\n",
    "            # En caso de que la unica opcion sea distinta\n",
    "            mispelled_candidates.append((word, candidatos))\n",
    "\n",
    "    return mispelled_candidates\n",
    "\n",
    "#print ( mispelled_and_candidates( words( target_txt[0] )))\n",
    "\n",
    "# Print misspelled words and candidates for each document in\n",
    "# target_txt  list\n",
    "for text in target_txt:\n",
    "    print ( mispelled_and_candidates ( words ( text ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correccion completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio supondremos que la primera palabra esta bien escrita y tiene sentido. A partir de la segunda palabra, consideraremos la funcion `next_word`, en caso de que esta arroje una palabra distinta y que podamos _\"considerar\"_ que existen posibles candidatos con `candidates`, entonces haremos el cambio a la palabra en `next_word` (en caso de que este en candidatos) o a la siguiente de las obtenidas por `candidatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_word(WORDS)\n",
    "dic = create_dict(words_from_file('big.txt'))\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    kw = known([word]) \n",
    "    kw1 = known(edits1(word)) \n",
    "    kw2 = known(edits2(word)) \n",
    "    \n",
    "    ret = set()\n",
    "    if kw or kw1:\n",
    "        if kw:\n",
    "            ret |= kw\n",
    "        if kw1:\n",
    "            ret |= kw1\n",
    "        if kw2:\n",
    "            ret |= kw2\n",
    "    else:\n",
    "        return [word]\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def maybe_bad(word, max_candidates=15):\n",
    "    \"\"\" Definimos algo como \"Posiblemente malo\" si\n",
    "    existen mas de una palabra que no esten tan \n",
    "    alejadas y se generen por candidates.\n",
    "    \"\"\"\n",
    "    candidatos = list(candidates(word))\n",
    "    candidatos.sort(key=lambda x: dist_lev(word, x))\n",
    "    if len(candidatos) > 1:\n",
    "        # En caso de que haya una opcion\n",
    "        return True, candidatos[:max_candidates]\n",
    "    elif len(candidatos) == 1 and word not in candidatos:\n",
    "        # En caso de que la unica opcion sea distinta\n",
    "        return True, candidatos\n",
    "    \n",
    "    return False, None\n",
    "    \n",
    "\n",
    "def spell_correction( input_text ):\n",
    "    corrected_text = [input_text[0]]\n",
    "\n",
    "    for iw in range(1, len(input_text)):\n",
    "        word = input_text[iw]\n",
    "        pword = corrected_text[iw-1]\n",
    "        nword = next_word(pword, dic)\n",
    "        mb = maybe_bad(word)\n",
    "        if mb[0] and nword != word:\n",
    "            if nword in mb[1]:\n",
    "                corrected_text.append(nword)\n",
    "            else:\n",
    "                corrected_text.append(mb[1][0])\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "        \n",
    "            \n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "tests = [['i', 'hav', 'a', 'ham'],\n",
    "     ['my', 'countr', 'is', 'biig'],\n",
    "     ['i', 'want', 't00', 'eat'],\n",
    "     ['the', 'science', '0ff', 'computer'],\n",
    "     ['the', 'science', 'off', 'computer'],\n",
    "     [ 'i', 'want' , 'too' , 'eat']\n",
    "    ]\n",
    "for s in tests:\n",
    "    # break\n",
    "    print(mispelled_and_candidates( s ))\n",
    "    print( spell_correction( s ))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word('want', dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates('t00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_txt = get_texts_from_catdir( './golden' )\n",
    "bad_txt = get_texts_from_catdir( './target' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f'<-------{i+1}------->')\n",
    "    for bw, gw in zip(words(target_txt[i]), words(golden_txt[i])):\n",
    "        if bw != gw:\n",
    "            print(f'bad =( {bw} != {gw} => {candidates(bw)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
